---
title: "Analysis of Raw GT3X files to Summary Measures in Chadwell et. al Data"
author: "John Muschelli"
date: '`r Sys.Date()`'
output: 
  bookdown::html_document2: 
    keep_md: true
    theme: cosmo
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: false
    number_sections: true
    code_download: yes
    code_folding: show
bibliography: refs.bib  
editor_options: 
  chunk_output_type: console
---

All code for this document is located at [here](https://raw.githubusercontent.com/muschellij2/osler/master/gt3x_limb_data/index.R).

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, comment = "", message = FALSE)
```

# Goal
The goal of this work is to map out a fully reproducible analysis of accelerometer data with open source tools, including the R programming language.  The data we will be using is publicly-available, as are all the R packages.  We wish to use this as a foundational teaching tool for those who have data from wearable devices, specifically accelerometers.  We will use types of accelerometer from ActiGraph.  Though many other manufacturers of accelerometers exist, such as Axtivity, Actical, or Shimmer, and have their own data formats, ActiGraph is the most popular devices used in research-grade settings.  Many other commercial devices, such as Fitbit or Apple Watches, do not provide raw accelerations or provide them at a much lower frequency than research grade devices.

The analysis will be setup as follows: setting up the environment to be able to read an analyze the data, describing the data, reading in the data, performing data exploration, checking, and cleaning, creating data summaries, and then analyzing the difference in summaries across groups.

# Setup and Packages

We will be using a number of packages for this analysis.  We will use `dplyr`, `tidyr`, `readr` for data manipulation and input, `lubridate` to manipulate time (which is crucial with accelerometer data).  The `read.gt3x` package from https://github.com/THLfi/read.gt3x will allow us to read in `gt3x` files from ActiGraph.  The `AGread` package and `pygt3x` package can read in these files as well, but may have more limited functionality or are less mature, but are actively developed.  The `SummarizedActigraphy` (https://github.com/muschellij2/SummarizedActigraphy/) package is a wrapper for getting multiple accelerometry and wearable data types into a common data format.  `SummarizedActigraphy` is the least mature of all the packages discussed here.

```{r packages, cache = FALSE}
# remotes::install_github("muschellij2/SummarizedActigraphy")
library(SummarizedActigraphy)
# remotes::install_github("THLfi/read.gt3x")
library(read.gt3x)
library(dplyr)
library(readxl)
library(tidyr)
library(readr)
library(lubridate)
library(kableExtra)
library(corrr)
library(readxl)
library(ggplot2)
library(zip)
```

If you run into any issues loading any of these packages, you will likely need to run `install.packages` to install them.  The few that are on GitHub and not on the main R package repository (CRAN), need to be installed with the `remotes::install_github` function, requiring you to pass in the `user/repository` combination rather than simply the package name.

# Data

The data is from @chadwell_kenney_granat_thies_galpin_head_2019.  The data consists of 40 subjects, 20 with prostheses, 20 without.  Each wore tri-axial ActiGraph devices for 7 days, one on each arm.  Information about the handedness dominance, arm of prosthetic, and other demographic information was also released.   We will provide the demographics and clinical information related to this population below (in Section \@ref(demographics-data)).

The data is publicly-available through Figshare (https://figshare.com/).  The Figshare project for this data is located at https://springernature.figshare.com/collections/Upper_limb_activity_of_twenty_myoelectric_prosthesis_users_and_twenty_healthy_anatomically_intact_adults_/4457855. The gt3x data is located at https://springernature.figshare.com/articles/Unprocessed_raw_30Hz_acceleration_data_stored_as_gt3x/7946189.  This data consists of one directory of GT3X files.  In order to download them from this project, you need to download the whole data set.  As the data are licensed under [CC0](https://creativecommons.org/publicdomain/zero/1.0/), we have copied the to another Figshare location (https://figshare.com/articles/dataset/Raw_GT3X_Data_Duplicate/11916087), where each `gt3x` file has been renamed without spaces or parentheses, each file can be downloaded separately, and each `gt3x` file has been compressed for faster transfer and smaller storage footprint.


## Downloading the Data

As the data is publicly available, you do not need to do any authentication to download the data.  You do, however, need to either 1) go to the original repository and download the full data set, 2) go to the Figshare page for the separated data and get the links for the files or download them via a browser, or 3) use the `rfigshare` package to obtain all the links to the data [@rfigshare].  As we wish to perform a fully reproducible pipeline, we will download the data programmatically.  In most cases of research, the data will be stored locally or shared on a centralized place, such as a computing cluster.   


First, we create an output directory for the data, then we get the details from the Figshare API (application programming interface).  

We specify `data_dir` to be the directory where the data will be downloaded; we use the `here` package to specify paths relative to the project directory.

```{r data_dir}
data_dir = here::here("data")
```

The details we are interested in are the download URL and name of the file, so we can download them locally.  Note, the complete data set is over 2.5 gigabytes of storage.  If storage is an issue, you could download each individual file, process the data set into smaller summary measures, and then delete the downloaded data.  After the code is run, we write out the data set of information to cache it for future use.



```{r get_fs_data}
outfile = here::here("data", "file_info.rds") # file of information
if (!file.exists(outfile)) {
  # pull figshare metadata
  x = rfigshare::fs_details("11916087", mine = FALSE, session = NULL)
  
  files = x$files
  # all_files is a tibble with one row per file
  files = lapply(files, function(x) {
    as.data.frame(x[c("download_url", "name", "id", "size")],
                  stringsAsFactors = FALSE)
  })
  all_files = dplyr::bind_rows(files)
  readr::write_rds(all_files, outfile)
} else {
  all_files = readr::read_rds(outfile)
}
head(all_files)
```

The `all_files` data set is all the files from that Figshare repository, which includes the `gt3x` files and the metadata with demographics and behavioral data.  Let's separate the meta data out from the rest of the gt3x files:

```{r showmeta}
meta = all_files %>% 
  filter(grepl("Meta", name))
meta
df = all_files %>% 
  filter(grepl("gt3x", name))
```

Now we see that we have one row per `gt3x` file:
```{r}
df %>% 
  head %>% 
  knitr::kable() %>% 
  kableExtra::kable_styling()
```

Below we are doing a series of data manipulations on the file name to create separate columns for the serial number, the identifier of the participant, the date of the start of recording.  We will then assign the group labels depending on the identifier of the participant. This process is standard data manipulation and does not pertain to accelerometry; we are using the popular `dplyr` package [@dplyr].  We will use this data set to download all the data into different sub-folders based on group.  The folder separation is not necessary as long as you have a table to map identifier to group status; this setup is simply a design choice.


```{r filedf}
df = df %>% 
  rename(file = name) %>% 
  # parsing the file name by _
  tidyr::separate(file, into = c("id", "serial", "date"), sep = "_",
                  remove = FALSE) %>% 
  mutate(date = sub(".gt3x.*", "", date)) %>% 
  mutate(date = lubridate::ymd(date)) %>% 
  # determining group by file name
  mutate(group = ifelse(grepl("^PU", basename(file)), 
                        "group_with_prosthesis",
                        "group_without_prosthesis")) %>% 
  mutate(article_id = basename(download_url)) %>% 
  # making an output file
  mutate(outfile = file.path(data_dir, group, basename(file)))
df %>% 
  head() %>% 
  knitr::kable() %>% 
  kableExtra::kable_styling()
```

The following will download all the files to their respective directories (separated by prosthetic group).  Again, see the note above about the size of the full data.  We present code below to download all the files, but later we will dynamically download one file to perform data exploration.

```{r real_dl, eval = FALSE}
# run only those where data doesn't exist
run_df = df %>% 
  filter(!file.exists(outfile))
for (iid in seq(nrow(run_df))) {
  idf = run_df[iid, ]
  out = curl::curl_download(idf$download_url, destfile = idf$outfile)
}
```

## Demographics data

Although we could create summaries and estimate differences by groups on accelerometry measures, we may want to adjust for other factors which may affect measures of activity, such as sex, age, body mass index (BMI), or if the device is on the dominant limb, if available.  Additionally, when analyzing those with prosthetics, information about the prosthetic or information about the length of use may be relevant.  The metadata with participant information is an `xlsx` file, so we will read it in using the `readxl` package.  You can also open this file in Excel and output it to a CSV file, but we do not recommend that as some formatting can change and we recommend against making intermediate files such as this if you can read in the raw data.  

```{r meta}
metadata = file.path(data_dir, "Metadata.xlsx")
if (!file.exists(metadata)) {
  out = download.file(meta$download_url, destfile = metadata)
}
meta = readxl::read_excel(metadata)
head(meta)
```

In this instance there are blank columns, which are given a naming convention with 2 periods (`..`) in front, we are simply making these columns `NA` and they will be discarded.

```{r metacol}
bad_col = grepl("^\\.\\.", colnames(meta))
colnames(meta)[bad_col] = NA
```

Also, as with many `xlsx` files, the first row may be the column identifier (it is in this case, but not always), but there may be additional sub-headers or merged cells before there is data.  Here, the first 2 rows are extra cells that need to be deleted, as well as some columns where the true column name is in one of the other rows than the first.  After some manipulation, each column should now have name that reflects the data in it:

```{r metahdr}
potential_headers = rbind(colnames(meta), meta[1:2, ])
potential_headers = apply(potential_headers, 2, function(x) {
  x = paste(na.omit(x), collapse = "")
  x = sub(" .csv", ".csv", x)
  x = sub(" .wav", ".wav", x)
  x = gsub(" ", "_", x)
  x
})
colnames(meta) = potential_headers
meta = meta[-c(1:2),]
colnames(meta)
```

In the Figshare project, the group from @chadwell_kenney_granat_thies_galpin_head_2019 uploaded multiple intermediate and ancillary files to provide varying levels of information for different audiences.  Some of this information included raw acceleration files (which we are replicate using `.gt3x` files), wear diaries, and 1-second summaries.  These columns in the `meta` object indicate if those files exist for that participant.

Here we are going to rename a column to `id` to harmonize it with the data from the `gt3x` files, recoding some non-ASCII characters, making some variables numeric, and inferring some values from group status:

```{r}
meta = meta %>% 
  rename(id = Participant_Identifier)
meta = meta %>% 
  filter(!is.na(id),
         id != "Participant Identifier") %>% 
  mutate_all(.funs = function(x) gsub("ü", "yes", x))
meta = meta %>% 
  mutate_at(
    .vars = vars(
      Age,
      `Time_since_prescription_of_a_myoelectric_prosthesis_(years)`
    ),
    readr::parse_number
  ) 
meta = meta %>% 
  mutate( `Time_since_limb_loss_(years)` = ifelse(
    `Time_since_limb_loss_(years)` == "Congenital", 0,
    `Time_since_limb_loss_(years)`)
  )
meta %>% 
  head %>% 
  knitr::kable() %>% 
  kableExtra::kable_styling()
```

We now have all data downloaded and the clinical or demographic information provided with the study.  We can now merge the demographic and clinical data with the file data set to have one data set to use throughout the analysis: 
```{r merge_metadata}
meta = meta %>% 
  dplyr::select(id, Gender, Age, `Absence_side_(*previously_dominant)`,
         Sensor_type, Right_Sensor, Left_Sensor, 
         `Time_since_limb_loss_(years)`, 
         `Time_since_prescription_of_a_myoelectric_prosthesis_(years)`)
meta = meta %>% 
  tidyr::gather(side, serial, Right_Sensor, Left_Sensor) %>% 
  mutate(side = sub("_Sensor", "", side))
df = full_join(meta, df)
```

```{r, echo = FALSE}
outfile = here::here("data", "file_info_with_metadata.rds")
readr::write_rds(df, outfile)
```

We can now show how to read the raw `gt3x` files to create metrics to combine with the demographic and clinical data.

# Reading in GT3X files

## What is a GT3X file?

This section may be technical, but is **important about removing participant identifiers when sharing data**.  If you want more information on the technical details on how the `.gt3x` format is read in, please visit https://github.com/actigraph/NHANES-GT3X-File-Format and https://github.com/actigraph/GT3X-File-Format.


At its core, a `.gt3x` file is simply a zip file.  You can unzip a `.gt3x` with any of the standard ways you'd extract a `.zip` file.  In some systems, it may not allow you to do this.  You should rename the `.gt3x` file to `.zip` and then you should be able to extract it.  Note, if the file name is `.gt3x.gz` then you have g-unzip the file first.  Even though we use "GT3X" generally, there are actually 2 different ways the data was stored, one referred to as the "old" format or the NHANES format.  The other format is generally referred to as a GT3X format or new format, but these semantics are not universal.  

Here we can show the steps of g-unzipping the `.gt3x` file, then running `unzip` to show the package contents. We will do everything in the temporary directory, as we will not need these files in this analysis because they already were extracted using functions to read in `.gt3x` files.

### Downloading one file

Here we will download one file.  Though the above code downloaded all the files, this code can be used if you do not want to download the whole data set and explore one file at a time.  NB: if you are using `download.file` instead of `curl_download`, make sure you set the `mode = "wb"` to download the data via a binary connection, otherwise we have seen issues, especially on Windows.

```{r dlone, echo=TRUE}
# set.seed(20200804)
# iid = sample(nrow(df), 1)
iid = 7
idf = df[iid, ]
print(idf$id)
gt3x_file = idf$outfile
if (!file.exists(gt3x_file)) {
  out = curl::curl_download(idf$download_url, destfile = gt3x_file)
}
id = idf$id
serial = idf$serial
```


```{r output, dependson="dlone", cache = FALSE}
output = R.utils::gunzip(gt3x_file, remove = FALSE, temporary = TRUE, overwrite = TRUE)
print(output)
zip::unzip(output, exdir = tempdir())
out = file.path(tempdir(), zip::zip_list(output)$filename)
print(out)
info_file = out[grepl("info.txt", out)]
```

In the "new" format, we see only 2 files, `info.txt`, which holds the metadata/header information and `log.bin`, which holds the "data".  In the older NHANES format, `info.txt` is still present, but the different types of data (activity, light, battery, etc.) are in separate files and the main file for activity is `actviity.bin`. 

We see there is potentially identifiable elements in `info.txt`:

```{r, cache = FALSE}
info = readLines(info_file)
info
```

We can edit these values to those that mask the protected data or remove them altogether.  You can remove the subject name (even though it's anonymized in this example):
```{r no_name, cache = FALSE}
info = info[!grepl("Subject Name", info)]
```

If we want to change the serial number, you should keep the first 3 letter prefix in the beginning because that may be used in the code for determining elements of reading in the data:

```{r new_serial, cache = FALSE}
serial = info[grepl("Serial", info)]
serial = sub(".*: ", "", serial)
nc = nchar(serial)
serial = substr(serial, 1,3)
new_serial = paste(sample(c(LETTERS, 0:9), nc - 3), collapse = "")
new_serial = paste0(serial, new_serial)
info[grepl("Serial", info)] = paste0("Serial Number: ", new_serial)
info
```


### Changing dates

Changing dates is a little more complicated and not really implemented right now.  We can show how to edit the metadata information below, but the true dates are embedded in the binary data itself, so it's much more involved to shift them.  Although some of these functions are not exported, we can show you a quick example:

```{r, cache = FALSE}
date_values = grepl("Date|Last Sample Time", info)
dates = info[date_values]
dates
dates = trimws(sub(".*(Date|Time): ", "", dates))
dates = read.gt3x:::ticks2datetime(dates)
dates
dates = dates - lubridate::as.period(38, unit = "days")
new_ticks = read.gt3x:::datetime2ticks(dates)
new_ticks
# check
dates == read.gt3x:::ticks2datetime(new_ticks)
```

Then we can put this in for the date data:
```{r new_date, cache = FALSE}
new_date_value = paste0(sub(": .*", ": ", info[date_values]), new_ticks)
info[date_values] = new_date_value
info
```

### Writing out the Meta Data
And then write out the info back to the same file, then zip the output to a proper `gt3x`.  We will read this in later to show the comparison to the original file:
```{r, cache = FALSE}
writeLines(info, info_file)
new_gt3x_file = tempfile(fileext = ".gt3x")
zip::zip(files = basename(out), zipfile = new_gt3x_file, root = tempdir(), include_directories = FALSE)
```




## Discussion of Package Options

THe `read.gt3x`, `AGread`, and `pygt3x` R packages can read gt3x files, but only `read.gt3x` and `pygt3x` packages can read in the "old" GT3X format from NHANES 2003-2006.  As `read.gt3x` is more mature and more thoroughly checked, we will use that package to read the `gt3x` format.  *Note, if you are using Python, the `gt3x` module can be used, which is the backbone for the `pygt3x` R package, which the author has helped develop).  If you need additional information, such as temperature, light information (referred to as Lux), etc, you may want to try `AGread::read_gt3x`.  Additionally, these packages can read in `gt3x` files that have been zipped, including gzipped (extension `gz`), bzipped (`bz2`), or xzipped (`xz`).

THe `SummarizedActigraphy::read_actigraphy` wraps the `read.gt3x` functionality, and puts the output format to the `AccData` format, which is common in the `GGIR` package [@GGIR], a popular analysis R package for accelerometer analysis.  The `read_actigraphy` also tries to read other formats, by using `GGIR::g.readaccfile` and other `GGIR` functionality.  


## Reading in one file

Here we will read in the one file we downloaded above.

Once the data is downloaded, you can read the file in using `read.gt3x`:
```{r gt3x_readin, dependson="dlone"}
gt3x_acc = read.gt3x(gt3x_file, verbose = FALSE,
                asDataFrame = TRUE, imputeZeroes = TRUE)
gt3x_acc
```

We see some information about the firmware, serial number, sampling rate/frequency, and the data: acceleration in gravity units ($g$) and the time of measurement.  As the sampling rate for this file is `r attributes(gt3x_acc)$sample_rate`, there are `r attributes(gt3x_acc)$sample_rate` measurements per second.  So, if you look at the `time` column, we see the time "repeated", but it's truly not the case.  The milliseconds are truly in there, but you need to change your settings to see them:

```{r readsecs}
options(digits.secs = 2)
gt3x_acc
```

We will show later, but functions from the `lubridate` package are immensely useful for manipulations of time variables; we will use the `floor_date` function to group all measurements within the same second together [@lubridate].   

Additional information, such as the header information can be found by extracting the attributes:

```{r attr}
at = attributes(gt3x_acc)
names(at)
at$header
```

This information is useful for checking the dates and times are appropriately parsed (e.g. stop date should be close (with 5 seconds) of when the recordings end)

```{r, include = FALSE}
rm(gt3x_acc)
```

Again, to harmonize the data output format (if different device manufacturers are aggregated), we will use `read_actigraphy`, which uses `read.gt3x`, but makes a different format, which is a list of values.  This design choice is that information such as the header are explicitly codified as opposed to attributes, which can be implicitly removed depending on the operation.  We set the header as a `tibble` so that standard `dplyr` functionality works on it.

```{r readin}
acc = read_actigraphy(idf$outfile, verbose = FALSE)
head(acc$data)
acc$freq # sample rate
acc$header
```

Now, this format may be more or less intuitive for your analysis.  If you prefer the output, simply use `read.gt3x`.  The `SummarizedActigraphy` package has the capability to read in other formats, such as Axivity CWA files, which were used in the [UK Biobank](https://www.ukbiobank.ac.uk/) study.  Thus, if you use `SummarizedActigraphy::read_actigraphy`, the code will not change when switching formats and the output should both have a `header` and `data` element.

Let's look at the number of measurements per second to ensure the reported sampling rate is the true sampling rate:

```{r sample_size, dependson="readin"}
res = acc$data %>% 
  mutate(dt = floor_date(time, "seconds")) %>% 
  group_by(dt) %>% 
  count()
table(res$n)
all(res$n == acc$freq)
```

Thus, we see that the sampling rate is indeed represented accurately in the data.  Note, in some instances, the last second may not have full data, which can be checked using the `tail` of `acc$data`.  Also, we aim to use functions that read the data in its rawest form.  This form includes data that may have variable sampling rates, which would show slightly different samples per second, (e.g. 99 or 101 records per second for 100Hz data).  Some readers perform implicit resampling, so be sure to check the code and documentation.  The `read.gt3x` function does not perform this and the device we are using has a fixed sampling rate.


## Comparison to a CSV

The group from @chadwell_kenney_granat_thies_galpin_head_2019 also uploaded the raw CSV files from ActiGraph's ActiLife software, which can be used to check to make sure our open-source software has read the data in correctly.  We will assume the output from ActiLife is the "gold standard".  

Similarly to the GT3X data set, the CSV files are given as [one directory of all data](https://springernature.figshare.com/articles/dataset/Unprocessed_raw_30Hz_acceleration_data_stored_as_csv/7946186) and we have separated them into [individual gzipped CSV files](https://figshare.com/articles/dataset/RAW_CSV_from_GT3X_Duplicate/12883463).  We will perform the same operation as above, but not download all the data as it essentially is the same size as the GT3X data.  We will download the data as needed to perform a comparison.

```{r figcsv}
x = rfigshare::fs_details("12883463", mine = FALSE, session = NULL)
files = x$files
files = lapply(files, function(x) {
  as.data.frame(x[c("download_url", "name", "id", "size")],
                stringsAsFactors = FALSE)
})
csv_df = dplyr::bind_rows(files)
csv_df = csv_df %>% 
  rename(file = name) %>% 
  tidyr::separate(file, into = c("id", "serial", "date"), sep = "_",
                  remove = FALSE) %>% 
  mutate(date = sub(".csv.*", "", date)) %>% 
  mutate(date = lubridate::ymd(date)) %>% 
  mutate(group = ifelse(grepl("^PU", basename(file)), 
                        "group_with_prosthesis",
                        "group_without_prosthesis")) %>% 
  mutate(article_id = basename(download_url)) %>% 
  mutate(outfile = file.path(data_dir, group, basename(file)))
csv_df %>% 
  head %>% 
  knitr::kable() %>% 
  kableExtra::kable_styling()
```

```{r csv_write, echo = FALSE}
csv_outfile = here::here("data", "csv_file_info.rds")
readr::write_rds(csv_df, csv_outfile)
```

Again, if you wish to download all the data, you can run the following code:

```{r real_csv_dl, eval = FALSE}
# run only those where data doesn't exist
run_df = csv_df %>% 
  filter(!file.exists(outfile))
for (iid in seq(nrow(run_df))) {
  idf = run_df[iid, ]
  out = curl::curl_download(idf$download_url, destfile = idf$outfile, quiet = FALSE)
}
```

Here we will extract the same ID and serial number that corresponds to the GT3X data.  The ID and serial together imply a unique identifier, as each participant wore 2 different accelerometers.  You should not assume that data from the right side and left side give the same or even similar values, especially depending on side dominance.  This data allows for that comparison, but we will not present that here.

```{r csv_dl_one}
# run only those where data doesn't exist
idf_csv = csv_df %>% 
  filter(id %in% idf$id, serial %in% idf$serial)
stopifnot(nrow(idf_csv) == 1)
csv_file = idf_csv$outfile
if (!file.exists(csv_file)) {
  out = curl::curl_download(idf_csv$download_url, destfile = csv_file, quiet = FALSE)
}
```

The CSV output from ActiLife is a CSV with the accelerometer, battery, and light data.  The CSV file, however, has a header of the first 10 lines:

```{r reaad_header}
readLines(csv_file, n = 11)
```

Multiple packages have implemented ways to read in the file.  For example, `AGread::read_AG_raw` will read in the data, but will perform processing and data aggregation.  THe `GGIR::read.myacc.csv` function will read a CSV, but has a number of options, which can be overwhelming.  The `SummarizedActigraphy::read_acc_csv` is a function that wraps the functionality of `readr::read_csv` and gives the raw output back.  We will make the output data a `tibble` for printing purposes:

```{r read_activity_csv}
csv = SummarizedActigraphy::read_acc_csv(csv_file, progress = FALSE)
csv$data = tibble::as_tibble(csv$data)
csv[c("header", "data")]
```
We see the information from what ActiGraph software version created the file, as well as information pertinent to the GT3X+ device.  We can compare this information from the header from the `.gt3x` file:


```{r show_header}
acc$header
```

And look at the parsed header information, which is in the `parsed_header` slot of the output of `read_acc_csv`:

```{r show_header2}
as.data.frame(csv$parsed_header)
```

We see additional information is given out from the `GT3X` file, or inferred by information such as the serial number.  This distinction is important for data sharing purposes.  If you wish to remove information from the `.gt3x` file, you should unzip the file as discussed above, edit the `info.txt` and then re-zip the file and rename it to `.gt3x`.  Care needs to be taken with certain elements of the header, such as dates and times, to ensure that something valid comes out when reading in the data.  Most commonly, any subject information is removed and dates may be kept or shifted.

### Simple Header Checks

Here we can do a simple data check against the known data from the demographics table:
```{r header}
hdr = acc$header 
hdr %>% 
  filter(Field %in% c("Sex", "Age", "Side", "Device Type", "Serial Number", "Dominance"))
idf[, c("Gender", "Age", "Sensor_type", "serial", "side")]
side = hdr$Value[hdr$Field == "Side"]
stopifnot(idf$side == side)
```

<!-- Some renaming of columns may be needed as the default is to give time as `HEADER_TIME_STAMP` to harmonize with the `MIMSunit` package, which we will discuss later. -->

## Comparing the CSV and GT3X

Here we will do a quick comparison of the CSV output and GT3X.  Let's do a simple check to see if they have the same number of rows:
```{r compare_csv_nrow}
stopifnot(nrow(acc$data) == nrow(csv$data))
```

Here we can see if all the X, Y, and Z values are the same:
```{r compare_csv}
xyz = c("X", "Y", "Z")
# see if they are the same
rs = unname(rowSums(acc$data[, xyz] == csv$data[, xyz]))
all(rs == 3)
```

Oh no!  They are different!  This difference is due to how ActiGraph conserves battery life, which is referred to as idle sleep mode.  This setting can be changed when setting up the accelerometer.  In idle sleep mode, the last observation is carried forward (LOCF) for all 3 axes.  The `fix_zeros` function allows us to replace these zeroes with the LOCF.    We see that after running `fix_zeros`, we get the same values from the CSV output.
```{r fix_zeros}
# these rows have all zeros
stopifnot(all(acc$data[which(rs < 3),xyz] == 0))
acc = fix_zeros(acc)
stopifnot(all(acc$data[, xyz] == csv$data[, xyz]))
```

The `AGread::read_gt3x` function, for new GT3X files, also has an option to flag these values. **NB: there have been observed zeroes for all 3 axes in ActiLife CSV outputs as well.  We are unaware when this can happen if idle sleep mode is enabled.  We recommend fixing zeros in that case as well**.  Also, most importantly, make sure any summary measure you are using takes into account for these values, discards these values, or sets them to missing as a true measure and this measure should not be treated equally.  One additional way is to use the variability as a measure, which we use with the Activity Index (AI) below, which was introduced by [@bai2016activity].

Now that we've shown the X/Y/Z values from the `.gt3x` function we read in with `R` is the same, we will no longer use the CSV outputs


# Plot the data

In order to plot the data, we will first transform the absolute date to the relative data compared to the first date observed (1-indexed, e.g. day 1, day 2, etc) as we only care about the day/time with respect to the start of measurement.  We'll subset the times between a half hour for plotting.

```{r p_diff, dependson="readin"}
res = acc$data %>% 
  mutate(day = floor_date(time, "day"),
         time = hms::as_hms(time)) %>% 
  mutate(day = difftime(day, day[1], units = "days") + 1) 
res = res %>% 
  filter(between(time, 
                 hms::as_hms("14:00:00"),
                 hms::as_hms("14:30:00"))
         ) 
head(res, 20)
```

We will transform the data into a "long" data set with respect to the 3 axes, so that we can easily plot all 3 axes.  We will use the function `gather` from the `tidyr` package, but future code should likely rely on the newer implementation of `pivot_longer` [@tidyr].  


```{r make_long}
res = res %>% 
  tidyr::gather(key = direction, value = accel, -time, -day)
head(res, 10)
```

We can arrange the data so that we see that the X/Y/Z data is represented in the data:

```{r arrange_long}
res = res %>% 
  arrange(day, time, direction)
head(res, 10)
```

Here we use the `ggplot2` package to plot the data across the 3 axes for the data subset based on the time constraints above, with each color for each axis [@ggplot2]. As there are many values for one day, we will plot the first day only

```{r plot, cache = FALSE}
g = res %>%
  filter(day == 1)  %>% 
  ggplot(aes(x = time, y = accel, colour = direction)) +
  geom_line() +
  theme(
  legend.background = element_rect(
    fill = "transparent"),
  legend.position = c(0.5, 0.9),
  legend.direction = "horizontal",
  legend.key = element_rect(fill = "transparent", 
                            color = "transparent") )  +
  ylab("Acceleration (g)") + 
  xlab("Time of Day")
print(g)
```

Additionally, we can facet the data for each axis:

```{r plot_facet}
print(g + facet_wrap(~ direction, ncol = 1))
```

We can perform the same data manipulations and plot, but for all the days observed.  We use the `%+%` operator to add the data set to the constructed plot.

```{r}
res = res %>% 
  filter(day %in% c(1:4))
gfacet = g + facet_wrap(~ day, ncol = 1) 
gfacet %+% res
```

Now we can observe the differences of activity of the same time, but across days.

# Non-wear estimation
Note, we have done no wear-time estimation yet.  The `accelerometry` package has a function `weartime` that creates an indicator of wear or non-wear.  The method in `accelerometry::weartime` is based on activity count values, which may or may not translate well for raw accelerometer values in gravity units.  We will not discuss that in detail here, but @syed2020novel go through a number of wear-time algorithms and have implemented them in the Python repository https://github.com/shaheen-syed/CNN-Non-Wear-Time-Algorithm, which some have been wrapped for R in https://github.com/muschellij2/weartime.  We will use the `weartime` package here to use the wear-time algorithm by van Hees et al. [@van2011estimation;@van2013separating], with the default parameters:

```{r weartime}
library(weartime)
wear = wt_hees_2013(acc$data, verbose = 2)
nrow(acc$data) == nrow(wear)
head(wear)
```

We see that the output `data.frame` is the same number of rows as the accelerometer data.  It can either be bound to the data or simply merged on the `time` variable.  A number of analysis steps can be done, including excluding these segments of time, setting them to missing, or "imputing" this data (not recommended).  There are a number of concerns with different methods, especially if different processing is done to the data, such as resampling to another sampling frequency.  We will simply change the values to missing in the following code, but will create a new object `wear_acc` to do a sensitivity analysis below of including the wear values or not:

```{r}
wear_acc = acc
wear_acc$data = full_join(wear_acc$data, wear)
# quick check to see if any NA
anyNA(wear_acc$data$wear)
wear_acc$data = wear_acc$data %>% 
  mutate(
    X = ifelse(!wear, NA_real_, X),
    Y = ifelse(!wear, NA_real_, Y),
    Z = ifelse(!wear, NA_real_, Z)
  ) %>% 
  select(-wear)
```

NB: this doubles the memory footprint of the analysis. 

A number of wear-time detection methods exist, namely that by @choi2011validation, which is implemented in the `wearingMarking` function of the [`PhysicalActivity`](https://cran.r-project.org/package=PhysicalActivity) package.  The method of @troiano2008physical is also popular.  Additionally, the [`actigraph.sleepr`](https://github.com/dipetkov/actigraph.sleepr) package implements these as well.  These methods were validated on 60-second intervals, and likely vector magnitude counts from ActiLife software, so they may need adapting to raw, sub-second, gravity-unit data.  

Moreover, it may be recommended to estimate non-wear periods and remove/put missing values in those regions, then calculate measures or calculate measures on the original data, then remove those that used non-wear data.  




# Data Analysis

Now most of the things we have shown so far use the raw, sub-second data.  Most analyses do not use this raw data, but calculate summary measures at the second, 5 second, minute, or day level.  Many decisions can be made on the order of operations, such as taking the mean of values within a second, then taking the standard deviation or taking the standard deviation of values within a second, then the mean, which are very different measures.  Overall, however, we will use the `calculate_measures` function from `SummarizedActigraphy` to calculate what we want.


### Calculating AI
If we look at the `calculate_measures` code, we see it calls other calculate functions:

```{r calculate_measures_code}
cat(as.character(body(calculate_measures))[6:12])
```

And we we can look at the definitions of these measures such as ` calculate_ai`, which estimates AI from @bai2016activity:
```{r calculate_measures}
body(calculate_ai)
```

This definition of  is a re-implementation of that from @bai2016activity, which is implemented in [`ActivityIndex`](https://cran.r-project.org/web/packages/ActivityIndex/index.html).  The method from @bai2016activity estimates a device-specific noise variance, which is commonly done by putting the device on a stationary device and measuring any variance.  As this study had not systematically performed this procedure, we use the version of AI where this variance is set to $0$.   One of the reasons we use this measure is that it has shown to track activiity similar to activity counts from ActiLife and has a simple, straightforward definition and interpretation. 


We see that the the first step is the group the data by 1-second increments, which is done using `floor_date`, then `group_by`, and then, for each second, the variance of X, Y, and Z are calculated.  Thus, each axis per-second variance is estimated; without sub-second data, this measure is zero.  The variances are averaged (divided by 3) and then the square root is taken.  The data has been reduced to second-level here.  Then, the data is grouped by the `unit`, which we will use as 1 minute, and the sum of these values are taken for all seconds in that minute.  Note, the `time` variable is actually a date-time object, so this is done for each day/time combination separately (i.e. nothing is done across days).  


Although we will focus on the AI for the analysis, we want to compare the AI measure and the other measures we calculate in this package.

```{r make_measures, dependson = c("readin", "fix_zeros", "dynamic_range")}
system.time({
  measures = calculate_measures(df = acc, fix_zeros = FALSE, 
                                calculate_mims = FALSE)
})
dim(measures)
head(measures)
system.time({
  wear_measures = calculate_measures(df = wear_acc, fix_zeros = FALSE, 
                                calculate_mims = FALSE)
})
ind = which(is.na(wear_measures$AI))[1]
ind = seq(ind - 2, ind + 8)
measures[ind,]
wear_measures[ind,]
```

We see the measures calculated are Activity Index (`AI`),  the mean Euclidean norm/angle (`mean_r`), Standard Deviation of the Euclidean norm (`SD`), Mean Absolute Deviation around the mean (`MAD`), Median Absolute Deviation around the mean (`MEDAD`), and.  Note, though AI calculates a second-level measures then sums them, all the other measures are calculated all data from that unit (1 minute) and summarized.  We have not subtracted $1$ gravity from any measure prior to or after calculation.


You can look up the definitions for the measures derived in `calculate_mad` as well.  The `calculate_mims` function calls `MIMSunit::mims_unit` function, which calculates MIMS (Monitor-Independent Movement Summary) of the data [@MIMSunit].  This function resamples the data to 100Hz data, which takes computational time and increasing the size of the data.  On this interpolated data, it also performs an extrapolation algorithm if the device hits its dynamic range.  By default, `calculate_measures` assumes the dynamic range of the device is $\pm 6$ g, but you should use the values from the header if available, or consult the device documentation:

```{r dynamic_range}
acc_ranges  = acc$header$Value[ acc$header$Field %in% c("Acceleration Min", "Acceleration Max")]
acc_ranges
acc_ranges = sort(as.numeric(acc_ranges))
```


### Calculating MIMS Units

We will calculate MIMS units with the `MIMSunit` package.  As we see below, this takes a significant amount of time.  Set `calculate_mims = TRUE` in `calculate_measures` only if you want to use this measure or compare it to the other measures (which we will show below).  

```{r MIMS, dependson = c("readin", "fix_zeros", "dynamic_range")}
library(MIMSunit)
system.time({
  mims = calculate_mims(acc, 
              dynamic_range = acc_ranges)
})
```

```{r merge_data}
mims = mims %>% 
  dplyr::rename(time = HEADER_TIME_STAMP)
measures = full_join(measures, mims)
```


```{r wear_MIMS, dependson = c("readin", "fix_zeros", "dynamic_range")}
library(MIMSunit)
system.time({
  wear_mims = calculate_mims(wear_acc, 
              dynamic_range = acc_ranges)
})
wear_mims = wear_mims %>% 
  dplyr::rename(time = HEADER_TIME_STAMP)
wear_measures = full_join(wear_measures, mims)
```

### Plotting Measures

We will use the [`ggplot2`](https://ggplot2.tidyverse.org/) package for plotting the data for different axes [@ggplot2].  In order to plot all measures on the same plot, we must reshape the data.  Again, we will use the function `gather` from the `tidyr` package, but future code should likely rely on the newer implementation of `pivot_longer` [@tidyr].  

```{r reshaper}
res = measures %>% 
  mutate(day = floor_date(time, "day"),
         time = hms::as_hms(time)) %>% 
  mutate(day = difftime(day, day[1], units = "days") + 1) 
res = res %>% 
  tidyr::gather(key = measure, value = value, -time, -day)
res = res %>% 
  arrange(day, time, measure)
head(res)
```

We see that each measure 

We will plot the data, looking only at the first day of values:
```{r plotmeas}
g = res %>%
  filter(day == 1)  %>% 
  ggplot(aes(x = time, y = value)) +
  geom_step() +
  theme(
  legend.background = element_rect(
    fill = "transparent"),
  legend.position = c(0.5, 0.9),
  legend.direction = "horizontal",
  legend.key = element_rect(fill = "transparent", 
                            color = "transparent") )  +
  xlab("Time of Day") + 
  facet_wrap(~ measure, ncol = 1, scales = "free_y")
print(g)
```


<!-- The `weartime` package is not ready for production yet but should be ready for use in the coming months. -->

We can take this plot and change the data to have days 2 through 5:
```{r}
res = res %>% 
  filter(day %in% c(2:5))
gfacet = g + 
  aes(colour = factor(as.numeric(day)), group = factor(as.numeric(day)))
gfacet %+% res
```

Where we see very different values at the same time at different days.


### Correlation of Measures

We can show the correlation of the measures with the others, noting that some of these have very high correlation.

```{r corr, dependson=c("make_measures", "merge_data", "MIMS")}
library(corrr)
measures %>% 
  select(-time) %>% 
  correlate() %>% 
  stretch(remove.dups = TRUE, na.rm = TRUE) %>% 
  arrange(desc(r))
```

```{r wcorr, dependson=c("make_measures", "merge_data", "MIMS")}
wear_measures %>% 
  select(-time) %>% 
  correlate(use = "pairwise.complete.obs") %>% 
  stretch(remove.dups = TRUE, na.rm = TRUE) %>% 
  arrange(desc(r))
```

And we see the same information in a correlation plot:

```{r corplot}
measures %>% 
  select(-time) %>% 
  correlate() %>%
  corrr::rplot(print_cor = TRUE, 
               colors = c("blue", "red"))
```


# Average Day 

Now we can create an average day profile.  We will calculate the mean value of these functions for each minute separately:
```{r avg}
to_minute = function(x) {
  x = format(x, "%H:%M:%S")
  x = hms::as_hms(x)
  x
}
average_day = measures %>% 
  mutate(time = to_minute(time)) %>% 
  group_by(time) %>% 
  summarise_at(vars(AI, SD, MAD, MEDAD), mean, na.rm = TRUE)
average_day %>%
  ggplot(aes(x = time, y = AI)) +
  geom_line()

average_day %>%
  ggplot(aes(x = time, y = MAD)) +
  geom_line()
```

## 1440 Format 

We can also make the data 1440 format:

```{r make1440}
measures1440 = measures %>% 
  select(time, AI) %>% 
  mutate(
    date = lubridate::as_date(time),
    time = to_minute(time)) %>% 
  mutate(time = sprintf("MIN_%04.0f", as.numeric(time)/60)) %>% 
  spread(time, value = AI)
head(measures1440)
```





# References
